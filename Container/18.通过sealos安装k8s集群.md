### Sealos安装k8s

#### 1. 准备服务器环境

```shell
# 为每台机器设置hostname
hostnamectl set-hostname xxx
```

#### 2. 第一台Master安装sealos

```shell
$ wget https://github.com/labring/sealos/releases/download/v4.1.3/sealos_4.1.3_linux_amd64.tar.gz \
   && tar zxvf sealos_4.1.3_linux_amd64.tar.gz sealos && chmod +x sealos && mv sealos /usr/bin
```

#### 3. 通过sealos安装k8s集群

```shell
# 通过sealos安装k8s集群，设置号需要安装的组件以及节点，注意版本号的选择
sealos run labring/kubernetes:v1.24.0 labring/helm:v3.8.2 labring/flannel:v0.18.1 \
     --masters 172.29.67.13,172.29.67.35,172.29.67.32 \
     --nodes 172.29.67.30
```

#### 4. 安装Ingress

```shell
# 通过sealos安装ingress
sealos run labring/ingress-nginx:4.1.0
```

#### 5. 安装k8s-dashboard

```shell
# 下载dashboard.yaml，并安装dashboard
curl https://raw.githubusercontent.com/kubernetes/dashboard/v2.6.1/aio/deploy/recommended.yaml -o dashboard.yaml

kubectl apply -f dashboard.yaml
```

```shell
# 生成证书文件，CN修改为自己的IP
openssl genrsa -out dashboard.key 2048
openssl req -days 3650 -new -out dashboard.csr -key dashboard.key -subj '/CN=172.29.67.13'
openssl x509 -req -in dashboard.csr -signkey dashboard.key -out dashboard.crt

# 替换之前的证书
kubectl delete secret kubernetes-dashboard-certs  --namespace kubernetes-dashboard 
kubectl create secret generic kubernetes-dashboard-certs --from-file=dashboard.key --from-file=dashboard.crt -n kubernetes-dashboard

# 然后需要删除kubernetes-dashboard的pod，让服务重新启动
```

```shell
# 通过Kubectl创建user，然后获取token，详细参考github，因k8s 1.24版本以后对token进行了一些修改，因此需要根据如下配置
curl https://github.com/shengyiwen/Document/blob/master/Container/Config/dashboard-user.yaml -o > dashboard-user.yaml

kubectl apply -f dashboard-user.yaml
# 获取dashboard-user的token
kubectl describe -nkube-system secret/secret-admin
```

#### 6. 安装promethus

```shell
# 通过sealos安装promethus和granfa，granfa默认账户名密码为admin/prom-operator
sealos run labring/kube-prometheus-stack:v0.56.0
# 修改promethus为NodePort类型，并设置为端口为30090和30091
```

#### 7. 安装Rancher

```shell
curl https://github.com/shengyiwen/Document/blob/master/Container/Config/create_signed_cert.sh -o create_signed_cert.sh
# 颁发签名，域名需要修改
./create_signed_cert.sh --ssl-domain=devbp.brain.com --ssl-size=2048 --ssl-date=3650
```

```shell
# 创建命名空间
kubectl create ns cattle-system
# 创建密钥
kubectl -n cattle-system create secret tls tls-rancher-ingress --cert=./tls.crt --key=./tls.key
# 创建密钥
kubectl -n cattle-system create secret generic tls-ca --from-file=./cacerts.pem
```

```shell
# 通过helm创建rancher
helm repo add rancher-stable https://releases.rancher.com/server-charts/stable

helm install rancher rancher-stable/rancher \
  --namespace cattle-system \
  --set systemDefaultRegistry=registry.cn-hangzhou.aliyuncs.com \
  --set hostname=devbp.brain.com \
  --set ingress.tls.source=secret \
  --set privateCA=true \
  --set bootstrapPassword=admin
  
# 随后检查ingress-nginx的pod中ingress-nginx-controller的日志
# 如出现此异常“ingress does not contain a valid IngressClass”
# 则需要修改cattle-system的ingress中rancher的配置，增加如下注释
# annotations:
#    kubernetes.io/ingress.class: nginx
# 随后再次检查日志，如果成功加载即可
```

```text
1. 查看ingress-nginx-controller所在的机器，并记录机器的ip地址，有可能在多台机器上运行
   kubectl get pod -o wide -n ingress-nginx
2. 查看ingress-nginx-controller对外暴露的端口号
   kubectl get svc ingress-nginx-controller -n ingress-nginx
3. 修改客户端的hosts文件，windows和linux有差异，增加如下配置：
   ingress-controllre所在的ip地址 devbp.brain.com
4. 浏览器打开: https://devbp.brain.com:对外暴露的端口号
```

#### 8. 安装minio

```shell
# 安装kubectl-minio
wget https://github.com/minio/operator/releases/latest/download/kubectl-minio_linux_amd64.zip
unzip kubectl-minio_linux_amd64.zip
cp kubectl-minio /usr/local/bin
sudo chmod +x /usr/local/bin/kubectl-minio

# 查看kubectl-minio版本
kubectl minio version
```

```shell
# 初始化minio，很多时候会下载不下来镜像，因此需要注意
kubectl minio init

# 查看安装情况 
kubectl get pods -n minio-operator

# 代理minio-operator以便访问
kubectl minio proxy -n minio-operator

# 如果遇到以下问题 unable to do port forwarding: socat not found
# 需要每台机器执行如下命令
yum install -y socat
```

```yaml
# sc的配置文件，minio-local-storage.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: sc-minio-local
provisioner: kubernetes.io/no-provisioner
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsume
```

```shell
# 创建sc
kubectl apply -f minio-local-storage.yaml
# 查看Sc
kubectl get sc
```

```shell
# 首先每台worker节点上创建挂在的文件夹目录
mkdir -p /data/minio-$HOSTNAME-01
```



```shell
# 创建PV的脚本，create_pv.sh

#!/bin/sh

help() {
  echo "--------------------------------"
  echo "usage: ./create_pv hostname1,hostname2 01 100"
  echo "param1: hosts list，split by ','. eg: 'hostname1,hostname2'"
  echo "param2: disk number. eg: '01, 02'"
  echo "param3: disk size with GB. eg: '100'"
  echo "--------------------------------"
}

case "$1" in
    -h|--help) help; exit;;
esac

if [[ $1 == '' ]];then
    help;
    exit;
fi

hosts=$1
diskNo=$2
diskSize=$3

print() {
echo "" > pv_$diskNo.yaml
for i in $(echo $hosts | sed "s/,/ /g")
do
  echo "
apiVersion: v1
kind: PersistentVolume
metadata:
   name: pv-${i}-${diskNo}
spec:
   capacity:
      storage: ${diskSize}Gi
   volumeMode: Filesystem
   accessModes:
   - ReadWriteOnce
   persistentVolumeReclaimPolicy: Retain
   storageClassName: sc-minio-local
   local:
      path: /data/minio-${i}-${diskNo}
   nodeAffinity:
      required:
         nodeSelectorTerms:
         - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
              - ${i}
---
" >> pv_${diskNo}.yaml
done
}

print
```

```shell
kubectl apply -f pv_01.yaml
```

```shell
wget https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.22/deploy/local-path-storage.yaml
# 1. 修改local-path-provisioner中的--debug，将其删除
# 2. 修改reclaimPolicy，将delete改为Retain
# 3. 搜索StorageClass，在metadata下增加annotaions，让其成为默认storageClass
#apiVersion: storage.k8s.io/v1
#kind: StorageClass
#metadata:
#  name: local-path
#  annotations:
#    storageclass.kubernetes.io/is-default-class: "true" # 默认storageclass
#provisioner: rancher.io/local-path
#volumeBindingMode: WaitForFirstConsumer
#reclaimPolicy: Retain
# 4. 修改ConfigMap中的默认路径地址
docker pull rancher/local-path-provisioner:v0.0.22
kubectl apply -f local-path-storage.yaml
```


```shell
kubectl create ns minio-tenant

kubectl minio tenant create minio-tenant --servers 2 --volumes 4 --capacity 200Gi --storage-class sc-minio-local --namespace minio-tenant --image minio/minio:RELEASE.2022-11-17T23-20-09Z

# 记录username and password
kubectl port-forward  --address 0.0.0.0 svc/minio-tenant-console -n minio-tenant 9443:9443

# 参考文件地址: http://javajun.net/posts/35914/index.html
```








